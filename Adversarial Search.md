## 极小极大算法
![[Pasted image 20240528171724.png]]
## α-β剪枝算法
![[Pasted image 20240528171710.png]]
## 蒙特卡洛树搜索MCTS
- 从当前局面的所有可落子点中随机选择一个点落子
- 重复以上过程
- 直到胜负可判断为止
- 经多次模拟后，选择胜率最大的点落子

基本思想:
- 将可能出现的状态转移过程用状态树表示
- 从初始状态开始重复抽样，逐步扩展树中的节点
- 父节点可以利用子节点的模拟结果，提高了效率
- 在搜索过程中可以随时得到行为的评价
## 信心上限算法UCB
![[Pasted image 20240528172231.png]]
### 信心上限的计算
![[Pasted image 20240528172349.png]]
## 信心上限树算法UCT
![[Pasted image 20240528172452.png]]
![[Pasted image 20240528172508.png]]
## AlphaGo
蒙特卡洛树搜索存在的问题  
1. 生成所有子节点，搜索范围过大
2. 模拟具有盲目性，有效模拟比例较低
AlphaGo 将神经网络与蒙特卡洛树搜索结合在一起
1. 缩小了搜索范围
2. 提高了模拟水平
### 策略网络
目标：像人类那样下棋
策略网络由一个神经网络构成
- 输入:当前棋局
	- $48$ 个通道，每个通道大小为 $19\times 19$
- 输出:棋盘上每个点的行棋概率
	- 概率越大越是好的行棋点
- 训练数据:  
	- $16$ 万盘人类棋手的数据
- 等效为一个分类问题  
	 - 任意一个棋局分类为 $361$ 类之一，人类行棋点为标记
 - 损失函数:
	 - $L(w)=-t_a\log(p_a)$
	- $t_a$ : 当前棋局下棋手落子在 $a$ 处时为 $1$ ，否则为 $0$ 
	- $p_a$ : 策略网络在 $a$ 出落子的概率
### 估值网络
估值网络由一个神经网络构成
- 输入:当前棋局  
	- $49$ 个通道，每个通道大小为 $19\times 19$
	- 比策略网络多一个通道
- 输出:当前棋局的收益
	- 收益的取值范围为$[-1, 1]$
- 训练样本
	- $16$ 万人类棋手的数据
- 等效为一个回归问题
	- 获胜时收益为 $1$ , 失败时收益为 $-1$
- 损失函数
	- $L(w)=(R-V(s))^{2}$
	- $\mathrm{R}$ 为棋局的胜负, 胜为 $1$  , 负为 $-1$
	- $\mathrm{V}(\mathrm{s})$ 为估值网络的输出，即预测的收益。
### 与蒙特卡洛树搜索融合
- MCTS 中的选择原则
	- 利用：收益好的节点
	- 探索：模拟次数少的节点
- AlphaGo 增加了第三个原则
	- 经验：落子概率高的节点
- 充分利用两个网络
![[Pasted image 20240616235843.png]]
![[Pasted image 20240616235910.png]]![[Pasted image 20240616235931.png]]
