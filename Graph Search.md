## DFS
## BFS
## 启发式搜索
### $A$ 算法
估价函数， $f(n)=g(n)+h(n)$ ， $h(n)$ 为启发函数
$f(n),g(n),h(n)$ 为 $f^*(n),g^*(n),h^*(n)$ 的估计值， $*$ 表示最优值的准确值
### $A^*$ 算法
若A算法满足 $h(n) \leq h^*(n)$ 则称为 $A^*$ 算法
#### 结论
- 定理（可采纳性定理）
	若存在从初始节点s到目标节点t有路径， 则 $A^*$ 必能找到最佳解结束。
- 定理
	设对同一个问题定义了两个 $A^*$ 算法 $A_1$ 和 $A_2$ ，若 $A_2$ 比 $A_1$ 有较多的启发信息， 即对所有非目标节点有 $h_2(n) > h_1(n)$ ，则 在具有一条从 $s$ 到 $t$ 的路径的隐含图上， 搜索结束时，由 $A_2$ 所扩展的每一个节点， 也必定由 $A_1$ 所扩展，即 $A_1$ 扩展的节点数至少和 $A_2$ 一样多。
	即：如果 $h_2(n) > h_1(n)$  (目标节点除外)， 则 $A_1$ 扩展的节点数 $\geq$ $A_2$ 扩展的节点数
	- 注：上述定理，评价指标是“扩展的节点数”，也就是说，同一个节点无论被扩展多少次，都只计算一次。
	- 这也是符合直觉的，若 $h$ 估计 $h^*$ 估计得更好，则需要扩展的节点数更少。
- 对 $h$ 的评价方法
	- 平均分叉数
		设共扩展了 $d$ 层节点，共搜索了 $N$ 个节点，则:$$N=\frac{1-b^{*(d+1)}}{1-b^*}$$其中， $b^*$ 称为平均分叉数。
		$b^*$ 越小说明 $h$ 效果越好。 
		实验表明， $b^*$ 是一个比较稳定的常数， 同一问题基本不随问题规模变化。
### $A^*$ 算法的改进
问题的提出:
因 $A$ 算法对 $m_l$ 类节点可能要重新放回到 OPEN 表中，因此可能会导致多次重复扩展同一个节点，导致搜索效率下降。
出现多次扩展节点的原因:
在前面的扩展中，并没有找到从初始节点到当前节点的最短路径
解决的途径:
对 $h$ 加以限制
- 能否对 $h$ 增加适当的限制，使得第一次扩展一个节点时，就找到了从 $s$ 到该节点的最短路径。
对算法加以改进
- 能否对算法加以改进，避免或减少节点的多次扩展。
#### 对 $h$ 加以限制
![[Pasted image 20240616163408.png]]
![[Pasted image 20240616163813.png]]
- 定理
	若 $h(n)$ 是单调的，则 $A^*$ 扩展了节点 $n$ 之后，就已经找到了到达节点 $n$ 的最佳路径。 
	即: 当 $A^*$ 选 $n$ 扩展时，有 $g(n)=g^*(n)$ 。
- 推论
	满足单调的 $h$ 一定满足 $A^*$ 条件。
#### 对算法加以改进
一些结论：
- OPEN表上任一具有 $f(n) < f^*(s)$ 的节点定会被 $A^*$ 扩展。
- $A^*$ 选作扩展的任一节点，定有 $f(n)\leq f^*(s)$ 。 
- 当 $h(n)$ 恒等于 $0$ 时， $h$ 为单调的。
将OPEN中 $f$ 值小于 $f^*(s)$ 的节点集合称作NEST，其中由于 $f^*(s)$ 的精确值暂时无法知晓，雇佣扩展过的所有节点的 $f$ 的最大值作为近似值。
对于NEST中的节点，我们将其 $h$ 令为 $0$ ，则 $h$ 为单调。使用 $f=g$ 值来选择扩展的节点
## Viterbi
![[Pasted image 20240616165407.png]]
![[Pasted image 20240616165416.png]]
